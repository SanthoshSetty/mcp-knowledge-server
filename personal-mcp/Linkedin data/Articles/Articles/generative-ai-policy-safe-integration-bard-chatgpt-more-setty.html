<html>
<head>
  <title>Generative AI Policy: Safe Integration of Bard, ChatGPT, and More in the Workplace</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
    <img class="series-logo" src="https://media.licdn.com/dms/image/v2/D4E12AQEqrG3yf687xg/series-logo_image-shrink_300_300/series-logo_image-shrink_300_300/0/1687008164072?e=1755734400&v=beta&t=_bAF5fqYM3fmXm02gQGDI9XnhC1lR7PLw5ic4nJyLe8" />
    <h3>
      <span class="series-title">WorkWonders with AI</span>
      </br>
      <span class="series-description">Boost office productivity with 'WorkWonders with AI', exploring AI tools like ChatGPT and Bard solving real problems</span>
    </h3>
    <img src="https://media.licdn.com/mediaD4E12AQELxZWQIVaVog" alt="Image generated using Midjourney and Canva" title="Image generated using Midjourney and Canva" />
      <h1><a href="https://www.linkedin.com/pulse/generative-ai-policy-safe-integration-bard-chatgpt-more-setty">Generative AI Policy: Safe Integration of Bard, ChatGPT, and More in the Workplace</a></h1>
    <p class="created">Created on 2023-07-26 19:37</p>
  <p class="published">Published on 2023-07-28 12:19</p>
  <div><p>Hundreds of millions of users utilize OpenAI's ChatGPT and Google's Bard daily. I use these engines every day. While generative artificial intelligence (AI) has gained immense popularity, its incorporation by businesses carries certain ethical concerns. It's essential for organizations to emphasize the responsible use of generative AI, ensuring it's accurate, secure, transparent, empowering, and sustainable. They should be aware of the ethical ramifications and act to minimize <strong>potential risks</strong>.&nbsp;</p><p>In this article, I will highlight specific instances where relying solely on generative AI, without additional research, can be <strong>detrimental</strong>. Additionally, I'll offer guidance on establishing a secure work environment with generative AI, clarify who should and shouldn't use it, and suggest best practices for its application.</p><h2>Issues with Generative AI</h2><p>First Lets discuss some of the obvious issues with Generative AI</p><h3><strong>1. Fabrication of Numbers and References</strong></h3><p>It's not uncommon for users, myself included, to encounter instances where engines like ChatGPT or Bard produce false data and references. A striking example comes from a few months ago when a lawyer naively used and submitted <a href="https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/" target="_blank">legal documentation sourced from ChatGPT</a>, which was later exposed as fabricated. This underscores the danger of relying blindly on generative AI outputs. To illustrate further, I posed a seemingly straightforward question to Bard regarding the utilization of generative AI and the percentage of companies deploying AI. Surprisingly, it provided an unsubstantiated answer, even when pressed for sources. This demonstrates the potential for even advanced AI systems to inadvertently misinform. Following are the screenshots of Bard generating fake numbers. </p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4D12AQEWt9omq5SffQ" src="https://media.licdn.com/dms/image/v2/D4D12AQEWt9omq5SffQ/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1690446555963?e=1755734400&amp;v=beta&amp;t=phuyJU0WZAXW-XEHyeqNQsOd4CPnFF0z3YP2BfkXJhA"><figcaption>Prompt to Bard to get the usage statistics on Generative AI</figcaption></figure><p><br></p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4D12AQHPjM_sW5sGZA" src="https://media.licdn.com/dms/image/v2/D4D12AQHPjM_sW5sGZA/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1690446568143?e=1755734400&amp;v=beta&amp;t=yEOM49cvdIwm5ckTkW7TCfz8_giAVHQpiBQDTCHc6ZY"><figcaption>Bard's response with Fake Numbers</figcaption></figure><p>The reference to a "46" in the BCG study is erroneous, especially considering <a href="https://www.bcg.com/capabilities/artificial-intelligence/generative-ai" target="_blank">that the study is from 2022</a>, a time when Generative AI like ChatGPT was not prominently recognized until its introduction in November.</p><p>This issue isn't new; many have addressed it previously. I had hoped that with the advent of GPT-4 and LaMDA, there might have been improvements in this regard. Sadly, that hasn't been the case. Thus, researchers relying solely on ChatGPT for numerical data or any other specific details are treading on thin ice unless they verify and cross-reference the provided information.</p><h3>2.Different types of Biases in Generative AI </h3><p>The training data behind these Large Language Models (LLMs) like ChatGPT or Bard can indeed carry biases. As a result, their outputs can sometimes reflect these biases, leading to skewed or discriminatory results.</p><h3>2.1 Gender Bias</h3><p>AI's potential bias against specific genders is a significant concern. For instance, if an AI system is trained on a predominantly male-centric dataset of resumes, it might show a propensity towards recommending male candidates for job positions. </p><p>My experience with Bard serves as a poignant illustration: when posed with a forward-looking query about which gender might dominate top jobs in 2050, Bard's response, as captured in the following screenshot, seems to go back to the <strong>stereotypical gender biased views from the 1950s</strong> rather than presenting an evolved or neutral stance. Such outputs underline the critical importance of addressing and rectifying biases in AI training data.  </p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQG2t2oKqqVxcw" src="https://media.licdn.com/dms/image/v2/D4E12AQG2t2oKqqVxcw/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1690542242265?e=1755734400&amp;v=beta&amp;t=SaGz2Ko_END9nFtturrLKjzm9WkBarWM_b3uEHCKpFg"><figcaption>Bard's response stereotyping Female roles</figcaption></figure><h3>2.2 <strong>Race Bias</strong></h3><p>AI can display racial biases based on its training data. For instance, if trained primarily on images of white individuals, the system might misidentify or misclassify individuals of other racial backgrounds.</p><h3>2.3 <strong>Class Bias</strong></h3><p>AI can inadvertently favor certain socioeconomic classes. For instance, if its training data primarily reflects the affluent, it might make prejudiced financial or lending decisions against those from less privileged backgrounds.</p><h3>2.4 <strong>Age Bias</strong></h3><p>AI can show age-based biases. A system trained predominantly on interactions with younger individuals might not be as adept at serving older customers, leading to potential service disparities.</p><h3>2.5 <strong>Disability Bias</strong></h3><p>AI systems might not be as inclusive of individuals with disabilities if their primary training data consists of able-bodied individuals. This can lead to misidentifications or other inappropriate responses when interacting with people with disabilities.</p><p>I could provide countless examples from ChatGPT or Bard showcasing biases with just a brief prompt, but space constraints in my article limit me.</p><p>It's imperative for AI developers and users to recognize these biases and take proactive steps to reduce them, promoting both fairness and inclusivity.</p><h3>3.Transparency and Privacy issues</h3><p>The prompts you submit and the responses you receive contribute to the training of these models. As such, exercise caution when sharing sensitive company details like confidential sales data, personal information, business strategies, or code. For example, <a href="https://uk.pcmag.com/news/146345/samsung-software-engineers-busted-for-pasting-proprietary-code-into-chatgpt" target="_blank">Samsung developers found</a> that the code they used with ChatGPT for debugging became publicly accessible. Likewise, some users managed to <a href="https://www.laptopmag.com/news/is-chatgpt-is-handing-out-free-windows-license-keys-no-but-yes-but-no" target="_blank">extract activation codes for Windows 10</a> . Your confidential company details, if inputted, might be used for model training, and subsequently, another user inquiring about your company could inadvertently access this information. This risk often goes unnoticed since users typically accept terms and conditions upfront without thorough scrutiny.</p><h2>Generative AI Risk assessment</h2><p>Given these considerations, ChatGPT and Bard can be used securely if approached with caution. Below is a risk assessment I've compiled from various sources, outlining which roles and industries can utilize Generative AI safely and which should avoid doing so.</p><figure class="slate-resizable-image-embed slate-image-embed__resize-full-width"><img alt="No alt text provided for this image" data-media-urn="urn:li:digitalmediaAsset:D4E12AQGl6NxTOoY5sw" src="https://media.licdn.com/dms/image/v2/D4E12AQGl6NxTOoY5sw/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1690543002052?e=1755734400&amp;v=beta&amp;t=FkRdq32jx38BxaqN1IlgxoUwhJckYgQEde3aaXnUB2k"><figcaption>AI Risk Meter for Roles and Industries</figcaption></figure><p>With all this information at hand, let's delineate the best and worst practices, shall we?</p><p><strong>Following are the 10 Tips to Safely Use Generative AI Engines:</strong></p><ol><li><strong>Assisted Coding and Debugging</strong>: When using AI for coding assistance or debugging, ensure that the code does not contain confidential comments, proprietary algorithms, or other sensitive information. Regularly review and sanitize the code snippets before submitting them to the AI.</li><li><strong>Grammatical Error Correction</strong>: Use the AI to proofread and correct grammatical mistakes in documents. However, make sure the documents are stripped of any personally identifiable information or sensitive data.</li><li><strong>Summarizing Texts</strong>: AI can efficiently summarize long texts, but always double-check the original content for any personal or confidential information before feeding it to the AI.</li><li><strong>Generating High-Level Ideas</strong>: Use generative AI to brainstorm or come up with novel concepts, ensuring that your prompts are generic and not revealing of any proprietary strategy or secret.</li><li><strong>Creating Document Structures</strong>: For tasks like drafting Product Requirement Documents (PRD), AI can help in laying out the structure or template. However, always review the final document to ensure no unintended information is included.</li><li><strong>Data Analysis without Personal Data</strong>: Use AI for analyzing datasets but ensure they are anonymized and stripped of any personal or sensitive data.</li><li><strong>General Knowledge Queries</strong>: Pose generic knowledge-based questions without referencing specific internal company data or strategies.</li><li><strong>Content Idea Brainstorming:</strong> Use AI to brainstorm topics or themes for content creation, such as blog posts or articles. While it's great for generating broad ideas, be cautious not to input or solicit company-specific details unless it's purposeful. </li><li><strong>Design Assistance</strong>: Use AI for design suggestions, but avoid uploading proprietary designs or artwork directly without necessary precautions.</li><li><strong>Training and Educational Purposes</strong>: Utilize AI for training modules or educational content, ensuring that the content doesn't unintentionally divulge company methods or strategies.</li></ol><p><br></p><p><strong><span>﻿</span>Following are the top 10 Notes to Companies on Enforcing Rules for a Safe Environment and Adhering to Privacy Laws:</strong></p><ol><li><strong>Transparent Policies</strong>: Ensure that all company policies related to data privacy and AI usage are transparent, easily accessible, and regularly updated to match the evolving landscape.</li><li><strong>Regular Training</strong>: Organize regular training sessions for employees, making them aware of the company's stance on data privacy, the safe use of AI, and its implications.</li><li><strong>AI-Generated Content Label</strong>: Require employees to clearly indicate if the content they've produced, whether internal or external, is generated or assisted by AI. This promotes transparency and accountability.</li><li><strong>Data Minimization</strong>: Encourage employees to only collect and process the minimum amount of personal data necessary for a specific purpose.</li><li><strong>Access Controls</strong>: Implement stringent access controls to sensitive data. Ensure that only authorized personnel have access to such data, especially when used in conjunction with AI tools.</li><li><strong>Strict NO to ChatGPT in Medical Research</strong>: Never rely on ChatGPT or similar AI platforms for medical research or advice. The complexities and nuances of the medical field demand human expertise.</li><li><strong>Avoid AI for Deeply Sensitive Issues</strong>: Refrain from delegating highly sensitive emotional, moral, or ethical matters to AI. Such topics require genuine human empathy and discernment.</li><li><strong>Never Substitute AI for True Creativity</strong>: While AI can support creative tasks, it shouldn't replace human originality and innovation. Encourage genuine creative thought and discourage over-reliance on AI for such endeavors.</li><li><strong>Regular Audits and Feedback</strong>: Conduct periodic audits of AI tools and platforms for potential privacy risks. Also, encourage employee feedback on AI use, updating rules and guidelines based on this input.</li><li><strong>Review Third-party Tools and Report Channels</strong>: When utilizing third-party AI tools, ensure they adhere to robust privacy standards. Also, establish clear channels for reporting potential AI-related concerns or breaches.</li></ol><p>By adhering to these guidelines, companies can responsibly integrate AI into their operations while ensuring the safety, privacy, and well-being of all stakeholders.</p><p><br></p><h2><strong>Wrapping it Up:</strong></h2><p>Alright, so here's the deal: ChatGPT, Bard, and their AI buddies are seriously cool tech tools with some mind-blowing capabilities. But just like that new gadget we unwrap and can't wait to use, it's essential we read the fine print. There's no denying their potential to change the game in countless industries, but we've got to keep our wits about us. Let's be savvy users, double-checking those outputs and understanding that sometimes, a human touch (and a bit of skepticism) can go a long way. After all, while AI might be smart, there's no replacing good ol' human intuition and judgment. Let's keep it real and use these tools wisely!</p><p><br></p><p><br></p><p><br></p></div>
</body>
</html>